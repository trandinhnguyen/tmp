{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with RetinaNet\n",
    "\n",
    "**Author:** [Srihari Humbarwadi](https://twitter.com/srihari_rh)<br>\n",
    "**Date created:** 2020/05/17<br>\n",
    "**Last modified:** 2023/07/10<br>\n",
    "**Description:** Implementing RetinaNet: Focal Loss for Dense Object Detection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Object detection a very important problem in computer\n",
    "vision. Here the model is tasked with localizing the objects present in an\n",
    "image, and at the same time, classifying them into different categories.\n",
    "Object detection models can be broadly classified into \"single-stage\" and\n",
    "\"two-stage\" detectors. Two-stage detectors are often more accurate but at the\n",
    "cost of being slower. Here in this example, we will implement RetinaNet,\n",
    "a popular single-stage detector, which is accurate and runs fast.\n",
    "RetinaNet uses a feature pyramid network to efficiently detect objects at\n",
    "multiple scales and introduces a new loss, the Focal loss function, to alleviate\n",
    "the problem of the extreme foreground-background class imbalance.\n",
    "\n",
    "**References:**\n",
    "\n",
    "- [RetinaNet Paper](https://arxiv.org/abs/1708.02002)\n",
    "- [Feature Pyramid Network Paper](https://arxiv.org/abs/1612.03144)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 10:33:03.321307: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-07 10:33:03.362712: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-07 10:33:03.362738: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-07 10:33:03.362765: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-07 10:33:03.370538: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.set_logical_device_configuration(\n",
    "    physical_gpus[0], [tf.config.LogicalDeviceConfiguration(memory_limit=30000)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the COCO2017 dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the entire COCO2017 dataset which has around 118k images takes a lot of time, hence we will be using a smaller subset of ~500 images for training in this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/srihari-humbarwadi/datasets/releases/download/v0.1.0/data.zip\n",
      "560525318/560525318 [==============================] - 42s 0us/step\n"
     ]
    }
   ],
   "source": [
    "url = \"https://github.com/srihari-humbarwadi/datasets/releases/download/v0.1.0/data.zip\"\n",
    "filename = os.path.join(os.getcwd(), \"data.zip\")\n",
    "keras.utils.get_file(filename, url)\n",
    "\n",
    "\n",
    "with zipfile.ZipFile(\"data.zip\", \"r\") as z_fp:\n",
    "    z_fp.extractall(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing utility functions\n",
    "\n",
    "Bounding boxes can be represented in multiple ways, the most common formats are:\n",
    "\n",
    "- Storing the coordinates of the corners `[xmin, ymin, xmax, ymax]`\n",
    "- Storing the coordinates of the center and the box dimensions\n",
    "  `[x, y, width, height]`\n",
    "\n",
    "Since we require both formats, we will be implementing functions for converting\n",
    "between the formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_xy(boxes):\n",
    "    \"\"\"Swaps the order of x and y coordinates of the boxes. While a tensor shape\n",
    "    is represented as (height, width), that of an image is represented as\n",
    "    (width, height). So this function will help.\n",
    "\n",
    "    Arguments:\n",
    "        boxes: A tensor with shape `(num_boxes, 4)` representing bounding boxes.\n",
    "\n",
    "    Returns:\n",
    "        swapped boxes with shape same as that of boxes\n",
    "    \"\"\"\n",
    "    return tf.stack([boxes[:, 1], boxes[:, 0], boxes[:, 3], boxes[:, 2]], axis=-1)\n",
    "\n",
    "\n",
    "def convert_to_xywh(boxes):\n",
    "    \"\"\"Changes the box format to center, width and height\n",
    "\n",
    "    Arguments:\n",
    "        boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[xmin, ymin, xmax, ymax]`\n",
    "\n",
    "    Returns\n",
    "        converted boxes with shape same as that of boxes\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [(boxes[..., :2] + boxes[..., 2:]) / 2, boxes[..., 2:] - boxes[..., :2]],\n",
    "        axis=-1,\n",
    "    )\n",
    "\n",
    "\n",
    "def convert_to_corners(boxes):\n",
    "    \"\"\"Changes the box format to corner coordinates\n",
    "\n",
    "    Arguments:\n",
    "        boxes: A tensor of rank 2 or higher with a shape of `(..., num_boxes, 4)`\n",
    "        representing bounding boxes where each box is of the format\n",
    "        `[x, y, width, height]`\n",
    "\n",
    "    Returns\n",
    "        converted boxes with shape same as that of boxes\n",
    "    \"\"\"\n",
    "    return tf.concat(\n",
    "        [boxes[..., :2] - boxes[..., 2:] / 2, boxes[..., :2] + boxes[..., 2:] / 2],\n",
    "        axis=-1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing pairwise Intersection Over Union (IOU)\n",
    "\n",
    "As we will see later in the example, we would be assigning ground truth boxes\n",
    "to anchor boxes based on the extent of overlapping. This will require us to\n",
    "calculate the Intersection Over Union (IOU) between all the anchor\n",
    "boxes and ground truth boxes pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_iou(boxes1, boxes2):\n",
    "    \"\"\"Computes pairwise IOU matrix for given two sets of boxes\n",
    "\n",
    "    Arguments:\n",
    "        boxes1: A tensor with shape `(N, 4)` representing bounding boxes where\n",
    "        each box is of the format `[x, y, width, height]`.\n",
    "        boxes2: A tensor with shape `(M, 4)` representing bounding boxes where\n",
    "        each box is of the format `[x, y, width, height]`.\n",
    "\n",
    "    Returns:\n",
    "        pairwise IOU matrix with shape `(N, M)`, where the value at i-th row,\n",
    "        j-th column holds the IOU between i-th box and j-th box from boxes1 and\n",
    "        boxes2 respectively.\n",
    "    \"\"\"\n",
    "    boxes1_corners = convert_to_corners(boxes1)\n",
    "    boxes2_corners = convert_to_corners(boxes2)\n",
    "\n",
    "    max_upper_left = tf.maximum(  # shape (N, M, 2)\n",
    "        boxes1_corners[:, None, :2],\n",
    "        boxes2_corners[:, :2],\n",
    "    )\n",
    "    min_lower_right = tf.minimum(  # shape (N, M, 2)\n",
    "        boxes1_corners[:, None, 2:],\n",
    "        boxes2_corners[:, 2:],\n",
    "    )\n",
    "\n",
    "    intersection = tf.maximum(0.0, min_lower_right - max_upper_left)  # shape (N, M, 2)\n",
    "    intersection_area = tf.reduce_prod(intersection, axis=-1)  # shape (N, M)\n",
    "\n",
    "    boxes1_area = tf.reduce_prod(boxes1[:, 2:], axis=-1)  # shape (N, )\n",
    "    boxes2_area = tf.reduce_prod(boxes2[:, 2:], axis=-1)  # shape (M, )\n",
    "\n",
    "    union_area = boxes1_area[:, None] + boxes2_area - intersection_area\n",
    "    return intersection_area / union_area\n",
    "\n",
    "\n",
    "def visualize_detections(\n",
    "    image, boxes, classes, scores, figsize=(7, 7), linewidth=7, color=[0, 0, 1]\n",
    "):\n",
    "    \"\"\"Visualize Detections\"\"\"\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    for box, _cls, score in zip(boxes, classes, scores):\n",
    "        text = f\"{_cls}: {score:.2f}\"\n",
    "        x1, y1, x2, y2 = box\n",
    "        w, h = x2 - x1, y2 - y1\n",
    "        patch = plt.Rectangle(\n",
    "            [x1, y1], w, h, fill=False, edgecolor=color, linewidth=linewidth\n",
    "        )\n",
    "        ax.add_patch(patch)\n",
    "        ax.text(\n",
    "            x1,\n",
    "            y1,\n",
    "            text,\n",
    "            bbox={\"facecolor\": color, \"alpha\": 0.4},\n",
    "            clip_box=ax.clipbox,\n",
    "            clip_on=True,\n",
    "        )\n",
    "    plt.show()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Anchor generator\n",
    "\n",
    "Anchor boxes are fixed sized boxes that the model uses to predict the bounding box for an object. It does this by regressing the offset between the location of the object's center and the center of an anchor box, and then uses the width and height of the anchor box to predict a relative scale of the object. In the case of RetinaNet, each location on a given feature map has nine anchor boxes (at three scales and three ratios).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnchorBox:\n",
    "    \"\"\"Generates anchor boxes\n",
    "\n",
    "    This class has operations to generate anchor boxes for feature maps at\n",
    "    strides `[8, 16, 32, 64, 128]`. Each anchor box is of the format\n",
    "    `[x, y, width, height]`\n",
    "\n",
    "    Attributes:\n",
    "        `aspect_ratios`: A list of float values representing the aspect ratios of\n",
    "        the anchor boxes at each location on the feature map.\n",
    "        `scales`: A list of float values representing the scale of the anchor boxes\n",
    "        at each location on the feature map.\n",
    "        `num_anchors`: The number of anchor boxes at each location on the feature\n",
    "        map.\n",
    "        `areas`: A list of float vales representing the areas of the anchor boxes\n",
    "        for each feature map in the feature pyramid.\n",
    "        `strides`: A list of float values representing the strides for each\n",
    "        feature map in the feature pyramid.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.aspect_ratios = [0.5, 1.0, 2.0]  # aspect ratio = width / height\n",
    "        self.scales = [2**x for x in [0, 1 / 3, 2 / 3]]\n",
    "        self.num_anchors = len(self.aspect_ratios) * len(self.scales)\n",
    "\n",
    "        self.strides = [2**i for i in range(3, 8)]\n",
    "        self.areas = [x**2 for x in [32, 64, 128, 256, 512]]\n",
    "        self.anchor_dims = self.compute_dims()\n",
    "\n",
    "    def compute_dims(self):\n",
    "        \"\"\"Computes anchor box dimensions for all ratios and scales at all levels\n",
    "        of the feature pyramid.\"\"\"\n",
    "        # This list will have 5 tensor of shape (1, 1, 9, 2)\n",
    "        anchor_dims_all = []\n",
    "        for area in self.areas:\n",
    "            anchor_dims = []\n",
    "\n",
    "            for ratio in self.aspect_ratios:\n",
    "                anchor_height = tf.math.sqrt(area / ratio)  # h.w = S <=> h.h.r = S\n",
    "                anchor_width = area / anchor_height\n",
    "\n",
    "                dims = tf.reshape(\n",
    "                    tf.stack([anchor_width, anchor_height], 0),\n",
    "                    shape=[1, 1, 2],\n",
    "                )\n",
    "\n",
    "                # After the loop, anchor_dims will receive 3 more tensor of shape (1, 1, 2)\n",
    "                for scale in self.scales:\n",
    "                    anchor_dims.append(scale * dims)\n",
    "            # At this point, anchor_dims will have 9 tensor of shape (1, 1, 2)\n",
    "\n",
    "            # anchor_dims will be converted to tensor of shape (1, 1, 9, 2)\n",
    "            anchor_dims_all.append(tf.stack(anchor_dims, axis=-2))\n",
    "\n",
    "        return anchor_dims_all  # array len 5\n",
    "\n",
    "    def _get_anchors(self, feature_height, feature_width, level):\n",
    "        \"\"\"Generates anchor boxes for a given feature map size and level\n",
    "\n",
    "        Arguments:\n",
    "            feature_height: An integer representing the height of the feature map\n",
    "            feature_width: An integer representing the width of the feature map\n",
    "            level: An integer representing the level of the feature map in the\n",
    "            feature pyramid\n",
    "\n",
    "        Returns\n",
    "            anchor boxes with the shape of\n",
    "            `(feature_height * feature_width * num_anchors, 4)`\n",
    "        \"\"\"\n",
    "        rx = tf.range(feature_width, dtype=tf.float32) + 0.5\n",
    "        ry = tf.range(feature_height, dtype=tf.float32) + 0.5\n",
    "\n",
    "        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1)  # shape (height, width, 2)\n",
    "        centers *= self.strides[level]\n",
    "        centers = tf.expand_dims(centers, axis=-2)  # shape (height, width, 1, 2)\n",
    "        centers = tf.tile(\n",
    "            centers,\n",
    "            [1, 1, self.num_anchors, 1],\n",
    "        )  # shape (height, width, 9, 2)\n",
    "\n",
    "        dims = tf.tile(\n",
    "            self.anchor_dims[level],\n",
    "            [feature_height, feature_width, 1, 1],\n",
    "        )  # shape (height, width, 9, 2)\n",
    "\n",
    "        # Each pixel (element) in the feature map will have 9 anchors and each\n",
    "        # anchor is represented by 4 values corresponding to its yc, xc, w, h\n",
    "        anchors = tf.concat([centers, dims], axis=-1)  # shape (height, width, 9, 4)\n",
    "        anchors = tf.reshape(\n",
    "            anchors,\n",
    "            [feature_height * feature_width * self.num_anchors, 4],\n",
    "        )\n",
    "        return anchors\n",
    "\n",
    "    def get_anchors(self, image_height, image_width):\n",
    "        \"\"\"Generates anchor boxes for all the feature maps of the feature pyramid.\n",
    "\n",
    "        Arguments:\n",
    "            image_height: Height of the input image\n",
    "            image_width: Width of the input image\n",
    "\n",
    "        Returns\n",
    "            anchor boxes for all the feature maps, stacked as a single tensor\n",
    "            with shape of `(total_anchors, 4)`\n",
    "        \"\"\"\n",
    "        anchors = [\n",
    "            self._get_anchors(\n",
    "                tf.math.ceil(image_height / stride),\n",
    "                tf.math.ceil(image_width / stride),\n",
    "                i,\n",
    "            )\n",
    "            for i, stride in enumerate(self.strides)\n",
    "        ]\n",
    "        anchors = tf.concat(anchors, axis=0)\n",
    "        return anchors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "\n",
    "Preprocessing the images involves two steps:\n",
    "\n",
    "- Resizing the image: Images are resized such that the shortest size is equal\n",
    "to 800 px, after resizing if the longest side of the image exceeds 1333 px,\n",
    "the image is resized such that the longest size is now capped at 1333 px.\n",
    "- Applying augmentation: Random scale jittering  and random horizontal flipping\n",
    "are the only augmentations applied to the images.\n",
    "\n",
    "Along with the images, bounding boxes are rescaled and flipped if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_flip_horizontal(image, boxes):\n",
    "    \"\"\"Flips image and boxes horizontally with 50% chance.\n",
    "\n",
    "    Arguments:\n",
    "        image: A 3-D tensor of shape `(height, width, channels)` representing\n",
    "        an image.\n",
    "        boxes: A tensor of shape `(num_boxes, 4)` representing bounding boxes\n",
    "        with normalized `(xmin, ymin, xmax, ymax)` format.\n",
    "\n",
    "    Returns:\n",
    "        Randomly flipped image and boxes.\n",
    "    \"\"\"\n",
    "    if tf.random.uniform([]) > 0.5:\n",
    "        image = tf.image.flip_left_right(image)\n",
    "        boxes = tf.stack(\n",
    "            [1 - boxes[:, 2], boxes[:, 1], 1 - boxes[:, 0], boxes[:, 3]],\n",
    "            axis=-1,\n",
    "        )\n",
    "    return image, boxes\n",
    "\n",
    "\n",
    "def resize_and_pad_image(\n",
    "    image, min_side=800, max_side=1333, jitter=[640, 1024], stride=128\n",
    "):\n",
    "    \"\"\"Resizes and pads an image while preserving aspect ratio.\n",
    "\n",
    "    1. Resizes an image so that the shorter side is equal to `min_side`.\n",
    "    2. If the longer side is greater than `max_side`, then resize the image with\n",
    "    longer side equal to `max_side`.\n",
    "    3. Pad with zeros on right and bottom to make the image shape divisible by\n",
    "    `stride`.\n",
    "\n",
    "    Arguments:\n",
    "        image: A 3-D tensor of shape `(height, width, channels)` representing an\n",
    "        image.\n",
    "        min_side: The shorter side of the image is resized to this value, if\n",
    "        `jitter` is set to None.\n",
    "        max_side: If the longer side of the image exceeds this value after\n",
    "        resizing, the image is resized such that the longer side now equals to\n",
    "        this value.\n",
    "        jitter: A list of floats containing minimum and maximum size for scale\n",
    "        jittering. If available, the shorter side of the image will be resized to\n",
    "        a random value in this range.\n",
    "        stride: The stride of the smallest feature map in the feature pyramid.\n",
    "        Can be calculated using `image_size / feature_map_size`.\n",
    "\n",
    "    Returns:\n",
    "        image: Resized and padded image.\n",
    "        image_shape: Shape of image before padding.\n",
    "        scaling_factor: Value used to resize image.\n",
    "    \"\"\"\n",
    "    image_shape = tf.cast(tf.shape(image)[:2], dtype=tf.float32)\n",
    "\n",
    "    if jitter:\n",
    "        min_side = tf.random.uniform(\n",
    "            [], minval=jitter[0], maxval=jitter[1], dtype=tf.float32\n",
    "        )\n",
    "\n",
    "    # Compute scaling factor\n",
    "    scaling_factor = min_side / tf.reduce_min(image_shape)\n",
    "    if scaling_factor * tf.reduce_max(image_shape) > max_side:\n",
    "        scaling_factor = max_side / tf.reduce_max(image_shape)\n",
    "\n",
    "    # Compute new image shape for resizing\n",
    "    image_shape *= scaling_factor\n",
    "    image = tf.image.resize(image, tf.cast(image_shape, dtype=tf.int32))\n",
    "\n",
    "    padded_image_shape = tf.cast(\n",
    "        tf.math.ceil(image_shape / stride) * stride,\n",
    "        dtype=tf.int32,\n",
    "    )\n",
    "    image = tf.image.pad_to_bounding_box(\n",
    "        image, 0, 0, padded_image_shape[0], padded_image_shape[1]\n",
    "    )\n",
    "    return image, image_shape, scaling_factor\n",
    "\n",
    "\n",
    "def preprocess_data(sample):\n",
    "    \"\"\"Applies preprocessing step to a single sample.\n",
    "\n",
    "    Arguments:\n",
    "        sample: A dict representing a single training sample.\n",
    "\n",
    "    Returns:\n",
    "        image: Resized and padded image with random horizontal flipping\n",
    "        applied.\n",
    "        bbox: bounding boxes with the shape `(num_objects, 4)` where each box\n",
    "        is of the format `[x, y, width, height]`.\n",
    "        class_id: An tensor representing the class ids of the objects, having\n",
    "        shape `(num_objects,)`.\n",
    "    \"\"\"\n",
    "    image = sample[\"image\"]\n",
    "    bbox = swap_xy(sample[\"objects\"][\"bbox\"])\n",
    "    class_id = tf.cast(sample[\"objects\"][\"label\"], dtype=tf.int32)\n",
    "\n",
    "    image, bbox = random_flip_horizontal(image, bbox)\n",
    "    image, image_shape, _ = resize_and_pad_image(image)\n",
    "\n",
    "    bbox = tf.stack(\n",
    "        [\n",
    "            bbox[:, 0] * image_shape[1],\n",
    "            bbox[:, 1] * image_shape[0],\n",
    "            bbox[:, 2] * image_shape[1],\n",
    "            bbox[:, 3] * image_shape[0],\n",
    "        ],\n",
    "        axis=-1,\n",
    "    )\n",
    "    bbox = convert_to_xywh(bbox)\n",
    "    return image, bbox, class_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding labels\n",
    "\n",
    "The raw labels, consisting of bounding boxes and class ids, need to be\n",
    "transformed into targets for training. This transformation consists of\n",
    "the following steps:\n",
    "\n",
    "- Generating anchor boxes for the given image dimensions\n",
    "- Assigning ground truth boxes to the anchor boxes\n",
    "- The anchor boxes that are not assigned any objects, are either assigned the\n",
    "  background class or ignored depending on the IOU\n",
    "- Generating the classification and regression targets using anchor boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
